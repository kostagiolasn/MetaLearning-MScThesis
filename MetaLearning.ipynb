{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import multiprocessing\n",
    "import os.path\n",
    "import csv\n",
    "import copy\n",
    "import joblib\n",
    "from torchvision import datasets\n",
    "import torchvision\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change USE_CUDA to True when running on GPU\n",
    "USE_CUDA  = False\n",
    "\n",
    "def w(v):\n",
    "    if USE_CUDA:\n",
    "        return v.cuda()\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p _cache\n",
    "cache = joblib.Memory(location='_cache', verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is responsible for disabling the propagation of certain variable gradients\n",
    "def detach_var(v):\n",
    "    var = w(Variable(v.data, requires_grad=True))\n",
    "    var.retain_grad()\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(optimizer_network, meta_optimizer, optimizee_obj_function, optimizee_network,\n",
    "        iterations_to_optimize, iterations_to_unroll, out_mul,\n",
    "        should_train = True):\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "    - optimizer_network (the optimizer network we use, here the LSTM)\n",
    "    - meta_optimizer (the optimizer of the optimizer network, e.g. Adam, SGD + nesterov, RMSprop, etc.)\n",
    "    - optimizee_obj_function (the optimizee's objective function)\n",
    "    - optimizee_network (the optimizee network)\n",
    "    - epochs (total epochs for training)\n",
    "    - iterations_to_optimize (iterations in every epoch)\n",
    "    - should_train (if should_train is True, then we just train the optimizer, else we evaluate)\n",
    "    \"\"\"\n",
    "    \n",
    "    if should_train:\n",
    "        optimizer_network.train()\n",
    "    else:\n",
    "        optimizer_network.eval()\n",
    "        unroll = 1\n",
    "\n",
    "    optimizee_obj_function = optimizee_obj_function(training=should_train)\n",
    "    optimizee = w(optimizee_network())\n",
    "    \n",
    "    # Counting the parameters of the optimizee\n",
    "    n_params = 0\n",
    "    for param in optimizee.parameters():\n",
    "        n_params += int(np.prod(param.size()))\n",
    "        \n",
    "    hidden_states = [w(Variable(torch.zeros(n_params, optimizer_network.hidden_size))) for _ in range(2)]\n",
    "    cell_states = [w(Variable(torch.zeros(n_params, optimizer_network.hidden_size))) for _ in range(2)]\n",
    "    \n",
    "    losses_list = []\n",
    "    \n",
    "    if should_train:\n",
    "        meta_optimizer.zero_grad()\n",
    "        \n",
    "    total_losses = None\n",
    "    \n",
    "    for iteration in range(iterations_to_optimize):\n",
    "        \n",
    "        # The loss of the current iteration\n",
    "        current_loss = optimizee(optimizee_obj_function)\n",
    "        \n",
    "        # Since the objective function of the optimizer is equal to the sum of the optimizee's losses\n",
    "        # we want to measure the loss of every iteration and add it to the total sum of losses\n",
    "        if total_losses is None:\n",
    "            total_losses = current_loss\n",
    "        else:\n",
    "            total_losses += current_loss\n",
    "            \n",
    "        losses_list.append(current_loss.data.cpu().numpy())\n",
    "        # Here dloss/dx is computed for every parameter x that has requires_grad = True\n",
    "        # These are accumulated into x.grad for every parameter x\n",
    "        # This is equal to x.grad += dloss/dx\n",
    "        \n",
    "        # We get the optimizee's gradients but we also retain the graph because\n",
    "        # we need to run backpropagation again when we optimize the optimizer\n",
    "        current_loss.backward(retain_graph = True)\n",
    "        \n",
    "        offset = 0\n",
    "        result_params = {}\n",
    "        \n",
    "        # These will be the new parameters. We will update all the parameters, cell and hidden states\n",
    "        # by iterating through the optimizee's \"all_named parameters\"\n",
    "        hidden_states2 = [w(Variable(torch.zeros(n_params, optimizer_network.hidden_size))) for _ in range(2)]\n",
    "        cell_states2 = [w(Variable(torch.zeros(n_params, optimizer_network.hidden_size))) for _ in range(2)]\n",
    "        \n",
    "        for name, param in optimizee.all_named_parameters():\n",
    "            current_size = int(np.prod(param.size()))\n",
    "            # We want to disconnect the gradients of some variables but not all, each time.\n",
    "            \n",
    "            # We do this in order to disconnect the gradients of the offset:offset+current_size\n",
    "            # parameters but still get the gradients of the rest.\n",
    "            gradients = detach_var(param.grad.view(current_size, 1))\n",
    "            \n",
    "            # Call the optimizer and compute the new parameters\n",
    "            updates, new_hidden, new_cell = optimizer_network(\n",
    "                gradients,\n",
    "                [h[offset:offset+current_size] for h in hidden_states],\n",
    "                [c[offset:offset+current_size] for c in cell_states]\n",
    "            )\n",
    "            \n",
    "            # Here we replace the old parameters with the new values\n",
    "            for i in range(len(new_hidden)):\n",
    "                hidden_states2[i][offset:offset+current_size] = new_hidden[i]\n",
    "                cell_states2[i][offset:offset+current_size] = new_cell[i]\n",
    "            \n",
    "            result_params[name] = param + updates.view(*param.size()) * out_mul\n",
    "            result_params[name].retain_grad()\n",
    "        \n",
    "        # If we have reached the number of iterations needed to update the optimizer\n",
    "        # we run backprop on the optimizer network\n",
    "        if iteration % iterations_to_unroll == 0:\n",
    "            if should_train:\n",
    "                # zero_grad() clears the gradients of all optimized tensors\n",
    "                meta_optimizer.zero_grad()\n",
    "                # we compute the gradient of the total losses  (i.e. the optimizer's loss function)\n",
    "                # with respect to the optimizer's parameters\n",
    "                total_losses.backward()\n",
    "                # we finally perform the optimization step, i.e. the updates\n",
    "                meta_optimizer.step()\n",
    "                \n",
    "            # Since we did the update on the optimizer network\n",
    "            # we overwrite the total_losses\n",
    "            total_losses = None\n",
    "            \n",
    "            # Here we detach the state variables because they are not propagated\n",
    "            # to the graph (see Figure 2 of the paper for details)\n",
    "            optimizee = w(optimizee_network(**{k: detach_var(v) for k, v in result_params.items()}))\n",
    "            hidden_states = [detach_var(v) for v in hidden_states2]\n",
    "            cell_states = [detach_var(v) for v in cell_states2]\n",
    "        else:\n",
    "            # Otherwise, we just create the next optimizee objective funtion\n",
    "            optimizee = w(optimizee_network(**result_params))\n",
    "            hidden_states = hidden_states2\n",
    "            cell_states = cell_states2\n",
    "            \n",
    "    return losses_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cache.cache\n",
    "def main_loop(optimizee_obj_function, optimizee_network, preprocessing = False,\n",
    "        epochs = 20, iterations_to_optimize = 100, iterations_to_unroll = 20, \n",
    "        n_tests = 100, lr = 0.001, out_mul = 1.0):\n",
    "    \n",
    "    optimizer_network = w(OptimizerNetwork(preprocessing = preprocessing))\n",
    "    \n",
    "    # To construct an Optimizer you need to give it an iterable containing the parameters to optimize\n",
    "    meta_optimizer = optim.Adam(optimizer_network.parameters(), lr = lr)\n",
    "    \n",
    "    # Initialize dummy variables for the best_net object and the best_loss\n",
    "    best_net = None\n",
    "    best_loss = 10000000000000000\n",
    "    \n",
    "    for _ in tqdm(range(epochs), 'epochs'):\n",
    "        for _ in tqdm(range(iterations_to_optimize), 'iterations'):\n",
    "            fit(optimizer_network, meta_optimizer, optimizee_obj_function, optimizee_network,\n",
    "                iterations_to_optimize, iterations_to_unroll, out_mul,\n",
    "                should_train = True)\n",
    "            \n",
    "        current_loss = (np.mean([\n",
    "                np.sum(fit(optimizer_network, meta_optimizer, optimizee_obj_function, optimizee_network,\n",
    "                iterations_to_optimize, iterations_to_unroll, out_mul,\n",
    "                should_train = False))\n",
    "                for _ in tqdm(range(n_tests), 'tests')\n",
    "               ]))\n",
    "        print(current_loss)\n",
    "        \n",
    "        if current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            best_net = copy.deepcopy(optimizer_network.state_dict())\n",
    "    \n",
    "    return best_loss, best_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first experiment we are going to use randomly generated $W$ and $y$. The matrix $W$ will be of dimensions 10x10 and will correspond to the weights that we want to learn, and the vector $y$ will be a 10-element vector that will represent the labels. Our optimizer will try to find a 10-element vector $\\theta$ that, when multiplied by $W$ will be as close as possible to y. Hence, our objective function that we want to minimize will be the squared error, i.e.:\n",
    "\n",
    "$$\n",
    "\\sum^{n}_{k = 1} (w_{i}^T \\cdot \\theta_{i} - y_{i})^{2}\n",
    "$$\n",
    "\n",
    "where $w_{i}$ is the $i$-th column vector of the $W$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomQuadraticLoss:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.W = w(Variable(torch.randn(10, 10)))\n",
    "        self.y = w(Variable(torch.randn(10)))\n",
    "    \n",
    "    def get_loss(self, theta):\n",
    "        return torch.sum((self.W.matmul(theta) - self.y) ** 2)\n",
    "    \n",
    "\n",
    "class QuadraticOptimizee(nn.Module):\n",
    "    def __init__(self, theta = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        if theta is None:\n",
    "            self.theta = nn.Parameter(torch.zeros(10))\n",
    "        else:\n",
    "            self.theta = theta\n",
    "            \n",
    "    def forward(self, target):\n",
    "        return target.get_loss(self.theta)\n",
    "    \n",
    "    def all_named_parameters(self):\n",
    "        return [('theta', self.theta)]\n",
    "    \n",
    "# Here we declare the optimizer. Here we have a parameter called preproc which is used in order to implement\n",
    "# what is described as \"Gradient preprocessing\" in Appendix A. Generally the input to the optimizer network is\n",
    "# a gradient which can get very high or very small values, especially when we're dealing with complex architectures.\n",
    "# Due to this, the optimizer training is susceptible to high variance in gradient values, as neural nets prefer to\n",
    "# deal with a relatively small range of values. Therefore a means of normalizing the gradients needs to be implemented.\n",
    "\n",
    "# The preprocessing factor is the p > 0 parameter in that paper section, which controls how small gradients are disregarded. It has\n",
    "# a default value of 10.0\n",
    "\n",
    "class OptimizerNetwork(nn.Module):\n",
    "    def __init__(self, preprocessing = False, hidden_size = 20, preprocessing_factor = 10.0):\n",
    "        super(OptimizerNetwork, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if preprocessing:\n",
    "            # Since we have the preprocessing flag enabled, we want the neural network\n",
    "            # to have two arguments and not just the gradient (see the forward function)\n",
    "            self.recurs = nn.LSTMCell(2, hidden_size)\n",
    "        else:\n",
    "            self.recurs = nn.LSTMCell(1, hidden_size)\n",
    "            \n",
    "        self.recurs2 = nn.LSTMCell(hidden_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.preprocessing = preprocessing\n",
    "        self.preprocessing_factor = preprocessing_factor\n",
    "        self.preprocessing_threshold = np.exp(-preprocessing_factor)\n",
    "        \n",
    "    def forward(self, inp, hidden, cell):\n",
    "        if self.preprocessing:\n",
    "            inp = inp.data\n",
    "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
    "            \n",
    "            # If the absolute value is greater or equal than the preprocessing threshold\n",
    "            # (see the condition in the first part of the gradient winged formula) we pass\n",
    "            # the log of the absolute value of the gradient divided by the preprocessing factor\n",
    "            # as the first parameter, and we pass the sign of the gradient as the second parameter.\n",
    "            keep_grads = torch.abs(inp) >= self.preprocessing_threshold\n",
    "            inp2[:, 0][keep_grads] = torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preprocessing_factor\n",
    "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads])\n",
    "            \n",
    "            # Else we pass -1 as the first parameter and then a scaled value of the gradient.\n",
    "            inp2[:, 0][~keep_grads] = -1\n",
    "            inp2[:, 1][~keep_grads] = float(np.exp(self.preprocessing_factor)) * inp[~keep_grads]\n",
    "            inp = w(Variable(inp2))\n",
    "            \n",
    "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n",
    "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n",
    "        \n",
    "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to try several values for the learning rate, in order to find the most promising one for our first experiment which is going to be to minimize the random quadratic loss we declared previously. We are going to use 20 epochs in order to find the best value for our learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3988b78c0fc144d8bb9764b41a942aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lr: 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eab92b40d95460e90f733c9985e024a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4476e876ad348da99b5d87a443d17a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bbd59d734040eba275ca4fd727f2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.48049e+07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f70eb57091f40109b8101448942026e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797f2348b30c4b81a2a2bdf0c0e89690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8249.89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2b10866d9e4e38b73537bd74cf6100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f48e1f43064416baa2226833dea575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54952.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5525e6de8eaf4143904e12f9d818884e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d4dd1c88504d388368200a0254c434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23963.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7cdcc24a1146a383e380846b7756c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27669aee90443e39a1d0d0ddff2fef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30830.6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5defe03f7326474d8a4e6064087347e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf380790ba1443998f8b6423ed5afbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4034.28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fade451034114204a31a1bbe3b8d16a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11978fe03a1342d4b897dde3bbd3119c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3838.52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6026e7581f5c47e1827931630741ab11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac3bfdae4d24534afc531c76787d738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3356.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36616bdaa27a4c6a9e2a6d6f05b23112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86901e80f24b473a8cdf17ed3814d350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d92fd44e9b415b8693726f5bcd5c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea498fc349c94cbc8bbc25f52539a021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-39b3d0255f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00003\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Testing lr:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomQuadraticLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuadraticOptimizee\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m_cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    470\u001b[0m                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                                   get_cached_func_info([func_id])['location']))\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmap_mode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m                 \u001b[0;31m# Memmap the output at the first call to be consistent with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verbose\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         self.store_backend.dump_item(\n\u001b[1;32m    704\u001b[0m             [func_id, args_id], output, verbose=self._verbose)\n",
      "\u001b[0;32m<ipython-input-21-241c6e6b5c9d>\u001b[0m in \u001b[0;36mmain_loop\u001b[0;34m(optimizee_obj_function, optimizee_network, preprocessing, epochs, iterations_to_optimize, iterations_to_unroll, n_tests, lr, out_mul)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0miterations_to_optimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations_to_unroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_mul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 should_train = False))\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tests'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                ]))\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-241c6e6b5c9d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0miterations_to_optimize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations_to_unroll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_mul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 should_train = False))\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tests'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                ]))\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-40678e89859d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(optimizer_network, meta_optimizer, optimizee_obj_function, optimizee_network, iterations_to_optimize, iterations_to_unroll, out_mul, should_train)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_named_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mcurrent_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;31m# We want to disconnect the gradients of some variables but not all, each time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2517\u001b[0m     return _methods._prod(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 2518\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_prod\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_prod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'all'):\n",
    "    print('Testing lr:', lr)\n",
    "    print(main_loop(RandomQuadraticLoss, QuadraticOptimizee, lr = lr)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to see what the best loss is for lr = 0.003 if we train for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_loss, best_quadratic_optimizer = main_loop(RandomQuadraticLoss, QuadraticOptimizee, lr = 0.003, n_epochs = 100)\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will find the best learning rate values for a selection of standard optimization algorithms (namely Adam, RMSprop, SGD and SGD with nesterov momentum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@cache.cache\n",
    "def fit_normal(optimizee_objective_function, optimizee_network, optimizer,\n",
    "               n_tests = 100, epochs = 100, **kwargs):\n",
    "    results = []\n",
    "    for i in tqdm(range(n_tests), 'tests'):\n",
    "        objective_function = optimizee_objective_function(training = False)\n",
    "        optimizee = w(optimizee_network())\n",
    "        optimizer = optimizer(optimizee.parameters(), **kwargs)\n",
    "        total_loss = []\n",
    "        for _ in range(epochs):\n",
    "            current_loss = optimizee(objective_function)\n",
    "            \n",
    "            total_loss.append(current_loss.data.cpu().numpy())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            current_loss.backward()\n",
    "            optimizer.step()\n",
    "        results.append(total_loss)\n",
    "    return results\n",
    "\n",
    "def find_best_lr_normal(optimizee_objective_function, optimizee_network, optimizer, **extra_kwargs):\n",
    "    best_loss = 10000000000000000.0\n",
    "    best_lr = 0.0\n",
    "    \n",
    "    for lr in tqdm([1.0, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001, 0.0003, 0.0001, 0.00003, 0.00001], 'Learning Rates'):\n",
    "        try:\n",
    "            loss = best_loss + 1.0\n",
    "            loss = np.mean([np.sum(s) for s in fit_normal(optimizee_objective_function, optimizee_network, optimizer, \n",
    "                                                          lr = lr, **extra_kwargs)])\n",
    "        except RunTimeError:\n",
    "            pass\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_lr = lr\n",
    "            \n",
    "    return best_loss, best_lr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OPTIMIZER_SELECTION = [(optim.Adam, {}), (optim.RMSprop, {}), (optim.SGD, {}),\n",
    "                       (optim.SGD, {'nesterov': True, 'momentum':0.9})]\n",
    "OPTIMIZER_NAMES = ['Adam', 'RMSprop', 'SGD', 'SGD + NestMom']\n",
    "\n",
    "for opt, kwargs in NORMAL_OPTS:\n",
    "    print(find_best_lr_normal(RandomQuadraticLoss, QuadraticOptimizee, opt, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUAD_LEARNING_RATES = [0.1, 0.03, 0.01, 0.01]\n",
    "fit_data = np.zeros((100, 100, len(OPTIMIZER_NAMES) + 1))\n",
    "\n",
    "for i, ((opt, extra_kwargs), lr) in enumerate(zip(NORMAL_OPTS, QUAD_LEARNING_RATES)):\n",
    "    np.random.seed(1234)\n",
    "    fit_data[:, :, i] = np.array(fit_normal(RandomQuadraticLoss, QuadraticOptimizee, opt, lr = lr, **extra_kwargs))\n",
    "    \n",
    "opt = w(OptimizerNetwork())\n",
    "opt.load_state_dict(best_quadratic_optimizer)\n",
    "np.random.seed(1234)\n",
    "fit_data[:, :, len(OPTIMIZER_NAMES)] = np.array([fit(opt, None, RandomQuadraticLoss, QuadraticOptimizee, \n",
    "                                                1, 100, 100, outmul = 1.0, should_train = False) for _ in range(100)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will plot the learning curves for the meta-optimized network (LSTM) vs the selection of the alternative standard algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = sns.tsplot(data = fit_data[:, :, :], condition = OPTIMIZER_NAMES + ['LSTM'], linestyle = '--', \n",
    "                color = ['r', 'b', 'g', 'k', 'y'])\n",
    "ax.lines[-1].set_linestyle('-')\n",
    "ax.legend()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('loss')\n",
    "plt.title(\"Comparison of learning curves between a selection of standard optimizers (dashed lines) and the meta-learned optimizer (straight line) in learning random quadratic functions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
